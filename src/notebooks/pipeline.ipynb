{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.repository.load_data import YoutubeVideoDataRepository\n",
    "from lib.service.data import YoutubeVideoDataService\n",
    "from lib.pipeline.feature import FeatureExtractionPipeline\n",
    "from lib.pipeline.training import TraingingPipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path2data = \"/Users/kori21/Google Drive/マイドライブ/projects/youtube-predict-view-count/data/30分以内に投稿された動画データ.xlsx\"\n",
    "path2data = \"../../data/30分以内に投稿された動画データ.xlsx\"\n",
    "repository = YoutubeVideoDataRepository(path2data=path2data)\n",
    "service = YoutubeVideoDataService(repository=repository)\n",
    "\n",
    "transformed_data = service.get_transformed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7e27853af84f94a193faa735ceb33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5347d8396a91478b97a3b3715f3fc6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dad5f24578740cb9baf1b40fa5d03ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4625e6a2d7741f9a307091506f769b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e390c7e52924ee1a9a18e81c2a13fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2200d2af105543d9b522e33788c21720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10439ac7d63042bc8402b56615770edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ec279bcb5d4bd1a822d4bbfe85e8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0445007e09d54bd3a2b8d5a698fb9020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3effa5c467a24e2ebb4aa3c638156450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e376916681d54b7c8e23d3f6b28cacce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d02815bc2241b1b8bb31d989c585a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unigram.json:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ec2518778245a7b27aedf7ffbd4b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kori21/Desktop/workspace/youtube_view_count_prediction/.venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "pipeline = FeatureExtractionPipeline(\n",
    "    original_df=transformed_data,\n",
    ")\n",
    "feature_extraxted_data = pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraxted_data.to_csv(\"../../data/feature_extraxted_data.csv\", index=False)\n",
    "# feature_extraxted_data = pd.read_csv(\"../../data/feature_extraxted_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minutes_diff', 'sin_hour', 'cos_hour', 'day_of_week_str_Friday', 'day_of_week_str_Monday', 'day_of_week_str_Saturday', 'day_of_week_str_Sunday', 'day_of_week_str_Thursday', 'day_of_week_str_Tuesday', 'day_of_week_str_Wednesday', 'comment_count', 'like_count', 'favorite_count', 'duration_min', 'subscriber_count', 'text_embeddings_0', 'text_embeddings_1', 'text_embeddings_2', 'text_embeddings_3', 'text_embeddings_4', 'text_embeddings_5', 'text_embeddings_6', 'text_embeddings_7', 'text_embeddings_8', 'text_embeddings_9', 'text_embeddings_10', 'text_embeddings_11', 'text_embeddings_12', 'text_embeddings_13', 'text_embeddings_14', 'text_embeddings_15', 'text_embeddings_16', 'text_embeddings_17', 'text_embeddings_18', 'text_embeddings_19', 'text_embeddings_20', 'text_embeddings_21', 'text_embeddings_22', 'text_embeddings_23', 'text_embeddings_24', 'text_embeddings_25', 'text_embeddings_26', 'text_embeddings_27', 'text_embeddings_28', 'text_embeddings_29', 'text_embeddings_30', 'text_embeddings_31', 'text_embeddings_32', 'text_embeddings_33', 'text_embeddings_34', 'text_embeddings_35', 'text_embeddings_36', 'text_embeddings_37', 'text_embeddings_38', 'text_embeddings_39', 'text_embeddings_40', 'text_embeddings_41', 'text_embeddings_42', 'text_embeddings_43', 'text_embeddings_44', 'text_embeddings_45', 'text_embeddings_46', 'text_embeddings_47', 'text_embeddings_48', 'text_embeddings_49', 'text_embeddings_50', 'text_embeddings_51', 'text_embeddings_52', 'text_embeddings_53', 'text_embeddings_54', 'text_embeddings_55', 'text_embeddings_56', 'text_embeddings_57', 'text_embeddings_58', 'text_embeddings_59', 'text_embeddings_60', 'text_embeddings_61', 'text_embeddings_62', 'text_embeddings_63', 'text_embeddings_64', 'text_embeddings_65', 'text_embeddings_66', 'text_embeddings_67', 'text_embeddings_68', 'text_embeddings_69', 'text_embeddings_70', 'text_embeddings_71', 'text_embeddings_72', 'text_embeddings_73', 'text_embeddings_74', 'text_embeddings_75', 'text_embeddings_76', 'text_embeddings_77', 'text_embeddings_78', 'text_embeddings_79', 'text_embeddings_80', 'text_embeddings_81', 'text_embeddings_82', 'text_embeddings_83', 'text_embeddings_84', 'text_embeddings_85', 'text_embeddings_86', 'text_embeddings_87', 'text_embeddings_88', 'text_embeddings_89', 'text_embeddings_90', 'text_embeddings_91', 'text_embeddings_92', 'text_embeddings_93', 'text_embeddings_94', 'text_embeddings_95', 'text_embeddings_96', 'text_embeddings_97', 'text_embeddings_98', 'text_embeddings_99', 'text_embeddings_100', 'text_embeddings_101', 'text_embeddings_102', 'text_embeddings_103', 'text_embeddings_104', 'text_embeddings_105', 'text_embeddings_106', 'text_embeddings_107', 'text_embeddings_108', 'text_embeddings_109', 'text_embeddings_110', 'text_embeddings_111', 'text_embeddings_112', 'text_embeddings_113', 'text_embeddings_114', 'text_embeddings_115', 'text_embeddings_116', 'text_embeddings_117', 'text_embeddings_118', 'text_embeddings_119', 'text_embeddings_120', 'text_embeddings_121', 'text_embeddings_122', 'text_embeddings_123', 'text_embeddings_124', 'text_embeddings_125', 'text_embeddings_126', 'text_embeddings_127', 'text_embeddings_128', 'text_embeddings_129', 'text_embeddings_130', 'text_embeddings_131', 'text_embeddings_132', 'text_embeddings_133', 'text_embeddings_134', 'text_embeddings_135', 'text_embeddings_136', 'text_embeddings_137', 'text_embeddings_138', 'text_embeddings_139', 'text_embeddings_140', 'text_embeddings_141', 'text_embeddings_142', 'text_embeddings_143', 'text_embeddings_144', 'text_embeddings_145', 'text_embeddings_146', 'text_embeddings_147', 'text_embeddings_148', 'text_embeddings_149', 'text_embeddings_150', 'text_embeddings_151', 'text_embeddings_152', 'text_embeddings_153', 'text_embeddings_154', 'text_embeddings_155', 'text_embeddings_156', 'text_embeddings_157', 'text_embeddings_158', 'text_embeddings_159', 'text_embeddings_160', 'text_embeddings_161', 'text_embeddings_162', 'text_embeddings_163', 'text_embeddings_164', 'text_embeddings_165', 'text_embeddings_166', 'text_embeddings_167', 'text_embeddings_168', 'text_embeddings_169', 'text_embeddings_170', 'text_embeddings_171', 'text_embeddings_172', 'text_embeddings_173', 'text_embeddings_174', 'text_embeddings_175', 'text_embeddings_176', 'text_embeddings_177', 'text_embeddings_178', 'text_embeddings_179', 'text_embeddings_180', 'text_embeddings_181', 'text_embeddings_182', 'text_embeddings_183', 'text_embeddings_184', 'text_embeddings_185', 'text_embeddings_186', 'text_embeddings_187', 'text_embeddings_188', 'text_embeddings_189', 'text_embeddings_190', 'text_embeddings_191', 'text_embeddings_192', 'text_embeddings_193', 'text_embeddings_194', 'text_embeddings_195', 'text_embeddings_196', 'text_embeddings_197', 'text_embeddings_198', 'text_embeddings_199', 'text_embeddings_200', 'text_embeddings_201', 'text_embeddings_202', 'text_embeddings_203', 'text_embeddings_204', 'text_embeddings_205', 'text_embeddings_206', 'text_embeddings_207', 'text_embeddings_208', 'text_embeddings_209', 'text_embeddings_210', 'text_embeddings_211', 'text_embeddings_212', 'text_embeddings_213', 'text_embeddings_214', 'text_embeddings_215', 'text_embeddings_216', 'text_embeddings_217', 'text_embeddings_218', 'text_embeddings_219', 'text_embeddings_220', 'text_embeddings_221', 'text_embeddings_222', 'text_embeddings_223', 'text_embeddings_224', 'text_embeddings_225', 'text_embeddings_226', 'text_embeddings_227', 'text_embeddings_228', 'text_embeddings_229', 'text_embeddings_230', 'text_embeddings_231', 'text_embeddings_232', 'text_embeddings_233', 'text_embeddings_234', 'text_embeddings_235', 'text_embeddings_236', 'text_embeddings_237', 'text_embeddings_238', 'text_embeddings_239', 'text_embeddings_240', 'text_embeddings_241', 'text_embeddings_242', 'text_embeddings_243', 'text_embeddings_244', 'text_embeddings_245', 'text_embeddings_246', 'text_embeddings_247', 'text_embeddings_248', 'text_embeddings_249', 'text_embeddings_250', 'text_embeddings_251', 'text_embeddings_252', 'text_embeddings_253', 'text_embeddings_254', 'text_embeddings_255', 'text_embeddings_256', 'text_embeddings_257', 'text_embeddings_258', 'text_embeddings_259', 'text_embeddings_260', 'text_embeddings_261', 'text_embeddings_262', 'text_embeddings_263', 'text_embeddings_264', 'text_embeddings_265', 'text_embeddings_266', 'text_embeddings_267', 'text_embeddings_268', 'text_embeddings_269', 'text_embeddings_270', 'text_embeddings_271', 'text_embeddings_272', 'text_embeddings_273', 'text_embeddings_274', 'text_embeddings_275', 'text_embeddings_276', 'text_embeddings_277', 'text_embeddings_278', 'text_embeddings_279', 'text_embeddings_280', 'text_embeddings_281', 'text_embeddings_282', 'text_embeddings_283', 'text_embeddings_284', 'text_embeddings_285', 'text_embeddings_286', 'text_embeddings_287', 'text_embeddings_288', 'text_embeddings_289', 'text_embeddings_290', 'text_embeddings_291', 'text_embeddings_292', 'text_embeddings_293', 'text_embeddings_294', 'text_embeddings_295', 'text_embeddings_296', 'text_embeddings_297', 'text_embeddings_298', 'text_embeddings_299', 'text_embeddings_300', 'text_embeddings_301', 'text_embeddings_302', 'text_embeddings_303', 'text_embeddings_304', 'text_embeddings_305', 'text_embeddings_306', 'text_embeddings_307', 'text_embeddings_308', 'text_embeddings_309', 'text_embeddings_310', 'text_embeddings_311', 'text_embeddings_312', 'text_embeddings_313', 'text_embeddings_314', 'text_embeddings_315', 'text_embeddings_316', 'text_embeddings_317', 'text_embeddings_318', 'text_embeddings_319', 'text_embeddings_320', 'text_embeddings_321', 'text_embeddings_322', 'text_embeddings_323', 'text_embeddings_324', 'text_embeddings_325', 'text_embeddings_326', 'text_embeddings_327', 'text_embeddings_328', 'text_embeddings_329', 'text_embeddings_330', 'text_embeddings_331', 'text_embeddings_332', 'text_embeddings_333', 'text_embeddings_334', 'text_embeddings_335', 'text_embeddings_336', 'text_embeddings_337', 'text_embeddings_338', 'text_embeddings_339', 'text_embeddings_340', 'text_embeddings_341', 'text_embeddings_342', 'text_embeddings_343', 'text_embeddings_344', 'text_embeddings_345', 'text_embeddings_346', 'text_embeddings_347', 'text_embeddings_348', 'text_embeddings_349', 'text_embeddings_350', 'text_embeddings_351', 'text_embeddings_352', 'text_embeddings_353', 'text_embeddings_354', 'text_embeddings_355', 'text_embeddings_356', 'text_embeddings_357', 'text_embeddings_358', 'text_embeddings_359', 'text_embeddings_360', 'text_embeddings_361', 'text_embeddings_362', 'text_embeddings_363', 'text_embeddings_364', 'text_embeddings_365', 'text_embeddings_366', 'text_embeddings_367', 'text_embeddings_368', 'text_embeddings_369', 'text_embeddings_370', 'text_embeddings_371', 'text_embeddings_372', 'text_embeddings_373', 'text_embeddings_374', 'text_embeddings_375', 'text_embeddings_376', 'text_embeddings_377', 'text_embeddings_378', 'text_embeddings_379', 'text_embeddings_380', 'text_embeddings_381', 'text_embeddings_382', 'text_embeddings_383']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 99249\n",
      "[LightGBM] [Info] Number of data points in the train set: 11133, number of used features: 398\n",
      "[LightGBM] [Info] Start training from score 15069.185035\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[104]\ttraining's rmse: 28758.7\tvalid_1's rmse: 45664.9\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 99249\n",
      "[LightGBM] [Info] Number of data points in the train set: 11133, number of used features: 398\n",
      "[LightGBM] [Info] Start training from score 14789.504716\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\ttraining's rmse: 11876\tvalid_1's rmse: 69736.3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 99249\n",
      "[LightGBM] [Info] Number of data points in the train set: 11134, number of used features: 398\n",
      "[LightGBM] [Info] Start training from score 15370.994701\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[781]\ttraining's rmse: 4352.7\tvalid_1's rmse: 35346.7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 99249\n",
      "[LightGBM] [Info] Number of data points in the train set: 11134, number of used features: 398\n",
      "[LightGBM] [Info] Start training from score 15804.828184\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's rmse: 58389.7\tvalid_1's rmse: 25934.9\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 99249\n",
      "[LightGBM] [Info] Number of data points in the train set: 11134, number of used features: 398\n",
      "[LightGBM] [Info] Start training from score 15283.722562\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[420]\ttraining's rmse: 8565.05\tvalid_1's rmse: 36879.3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 99249\n",
      "[LightGBM] [Info] Number of data points in the train set: 13917, number of used features: 398\n",
      "[LightGBM] [Info] Start training from score 15263.659050\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[174]\ttraining's rmse: 13826.1\tvalid_1's rmse: 44562.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kori21/Desktop/workspace/youtube_view_count_prediction/.venv/lib/python3.10/site-packages/mlflow/data/digest_utils.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  string_columns = trimmed_df.columns[(df.applymap(type) == str).all(0)]\n",
      "2024/01/24 19:58:10 WARNING mlflow.data.pandas_dataset: Failed to infer schema for Pandas dataset. Exception: Unable to map 'object' type to MLflow DataType. object can be mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024/01/24 19:58:12 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /var/folders/rm/j44x8xm10773j936hpjjq90h0000gn/T/tmpvax2adgf/model, flavor: lightgbm), fall back to return ['lightgbm==4.2.0']. Set logging level to DEBUG to see the full traceback.\n",
      "/Users/kori21/Desktop/workspace/youtube_view_count_prediction/.venv/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'lgb_base_model'.\n",
      "2024/01/24 19:58:12 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: lgb_base_model, version 1\n",
      "Created version '1' of model 'lgb_base_model'.\n"
     ]
    }
   ],
   "source": [
    "training_pipeline = TraingingPipeline(\n",
    "    feature_extracted_df=feature_extraxted_data\n",
    ")\n",
    "training_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
